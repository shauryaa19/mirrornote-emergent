# ğŸ¤ The Mirror Note

**AI-Powered Voice Assessment Platform**

> Discover your voice, elevate your communication skills, and unlock your full potential as a speaker with personalized AI-driven insights.

[![Built with Expo](https://img.shields.io/badge/Built%20with-Expo-000020.svg?style=flat&logo=expo)](https://expo.dev/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![MongoDB](https://img.shields.io/badge/MongoDB-47A248?style=flat&logo=mongodb&logoColor=white)](https://www.mongodb.com/)
[![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=flat&logo=openai&logoColor=white)](https://openai.com/)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Architecture](#-architecture)
- [Getting Started](#-getting-started)
- [Project Structure](#-project-structure)
- [API Documentation](#-api-documentation)
- [Testing](#-testing)
- [Production Integration](#-production-integration)
- [Deployment](#-deployment)
- [Contributing](#-contributing)

---

## ğŸŒŸ Overview

**The Mirror Note** is a revolutionary mobile application that analyzes your voice to provide actionable insights on your speaking style, clarity, confidence, and communication effectiveness. Using cutting-edge AI technologies (OpenAI Whisper for transcription and GPT-4 for analysis), the app helps users improve their public speaking, presentation skills, and overall communication abilities.

### Why The Mirror Note?

- ğŸ¯ **Personalized Feedback**: Get detailed insights tailored to your unique voice profile
- ğŸ“Š **Comprehensive Analysis**: Track metrics like speaking pace, filler words, pitch, clarity, and confidence
- ğŸ§  **AI-Powered Coaching**: Receive personalized training questions to improve weak areas
- ğŸ“± **Mobile-First**: Practice and analyze your voice anytime, anywhere
- ğŸ¨ **Beautiful UI**: Sage green themed design for a calming, professional experience

---

## âœ¨ Features

### Core Functionality

#### ğŸ™ï¸ Voice Recording
- **Two Recording Modes**: Free Speaking & Guided Practice (up to 2 minutes)
- Real-time waveform visualization
- Microphone permission handling

#### ğŸ§ª AI-Powered Analysis
- **Speech Transcription**: OpenAI Whisper for accurate transcription
- **Voice Metrics**: Speaking pace (WPM), filler word detection, pitch analysis (Hz)
- **Scoring**: Overall (0-100), Clarity, Confidence, Tone
- **Voice Archetype**: Identifies unique speaking style (e.g., "Warm Storyteller")

#### ğŸ“ˆ Comprehensive Results
- Visual charts for metrics (bar charts, donut charts)
- Detailed filler words breakdown
- Strengths and improvement areas
- Pitch visualization

#### ğŸ“ Personalized Training
- AI-generated training questions based on your analysis
- Free tier: 3 questions per assessment
- Premium tier: 10 questions with detailed answers

#### ğŸ’ Premium Features
- Subscription plans: Monthly (â‚¹499), Yearly (â‚¹3,999)
- Unlimited assessments
- Access to all training questions
- Razorpay payment integration (production-ready)

#### ğŸ‘¤ User Management
- Simple email/name authentication (current)
- Google OAuth ready (see integration guide)
- Assessment history tracking

---

## ğŸ› ï¸ Tech Stack

### Frontend
| Technology | Version | Purpose |
|------------|---------|----------|
| React Native | 0.79 | Cross-platform mobile framework |
| Expo | 54 | Development platform & build tools |
| TypeScript | 5.8 | Type-safe development |
| Expo AV | 16 | Audio recording & playback |
| Axios | 1.13 | HTTP client for API calls |
| Expo Router | 5 | File-based navigation |

### Backend
| Technology | Version | Purpose |
|------------|---------|----------|
| FastAPI | 0.110 | High-performance Python web framework |
| Python | 3.10+ | Backend programming language |
| Motor | 3.3 | Async MongoDB driver |
| OpenAI API | 2.7 | Whisper + GPT-4 |
| Pydantic | 2.12 | Data validation |
| Uvicorn | 0.25 | ASGI server |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Mobile App (React Native/Expo)      â”‚
â”‚  Dashboard â†’ Recording â†’ Processing â†’ Resultsâ”‚
â”‚                      â†“ (Axios)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            FastAPI Backend                  â”‚
â”‚  /api/analyze-voice, /api/assessment/{id}   â”‚
â”‚          â†“            â†“            â†“         â”‚
â”‚     OpenAI      MongoDB       Razorpay      â”‚
â”‚  Whisper+GPT4   (Motor)       (Prod)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow**: Recording â†’ Base64 Upload â†’ Whisper Transcription â†’ GPT-4 Analysis â†’ MongoDB Storage â†’ Results Display

---

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** 18+ - [Download](https://nodejs.org/)
- **Python** 3.10+ - [Download](https://www.python.org/)
- **MongoDB** - [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) or local
- **OpenAI API Key** - [Get Key](https://platform.openai.com/)
- **Expo CLI** - `npm install -g expo-cli`

### Backend Setup

```bash
# 1. Navigate to backend
cd backend

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Create .env file
cat > .env << EOF
MONGO_URL=mongodb+srv://username:password@cluster.mongodb.net/
DB_NAME=mirrornote
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx
EOF

# 5. Run server
uvicorn server:app --reload --host 0.0.0.0 --port 8000
```

Backend available at: `http://localhost:8000` | Docs: `http://localhost:8000/docs`

### Frontend Setup

```bash
# 1. Navigate to frontend
cd frontend

# 2. Install dependencies
npm install  # or yarn install

# 3. Create .env file
echo "EXPO_PUBLIC_BACKEND_URL=http://localhost:8000" > .env

# For physical device testing:
# Replace localhost with your computer's IP (find via ipconfig/ifconfig)
# Example: EXPO_PUBLIC_BACKEND_URL=http://192.168.1.100:8000

# 4. Start Expo
npx expo start

# 5. Run on device/emulator
# Option A: Scan QR with Expo Go app (recommended)
# Option B: Press 'i' for iOS simulator (macOS only)
# Option C: Press 'a' for Android emulator
# Option D: Press 'w' for web browser
```

---

## ğŸ“ Project Structure

```
mirrornote-emergent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.py              # FastAPI app with all endpoints
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ .env                   # Environment variables (create)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ (tabs)/           # Tab navigation
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard.tsx # Main dashboard
â”‚   â”‚   â”‚   â”œâ”€â”€ history.tsx   # Assessment history
â”‚   â”‚   â”‚   â””â”€â”€ profile.tsx   # User profile
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â””â”€â”€ login.tsx     # Login screen
â”‚   â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”‚   â””â”€â”€ AuthContext.tsx # Auth state
â”‚   â”‚   â”œâ”€â”€ constants/
â”‚   â”‚   â”‚   â””â”€â”€ theme.ts      # Design system
â”‚   â”‚   â”œâ”€â”€ index.tsx         # Splash screen
â”‚   â”‚   â”œâ”€â”€ recording.tsx     # Voice recording
â”‚   â”‚   â”œâ”€â”€ processing.tsx    # Analysis processing
â”‚   â”‚   â”œâ”€â”€ results.tsx       # Results display
â”‚   â”‚   â””â”€â”€ payment.tsx       # Subscription
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env                  # Environment variables (create)
â”‚
â”œâ”€â”€ PRODUCTION_INTEGRATION_GUIDE.md  # Google Auth & Razorpay
â”œâ”€â”€ QUICK_START_PRODUCTION.md        # Quick setup guide
â”œâ”€â”€ test_result.md                   # Testing documentation
â””â”€â”€ README.md                        # This file
```

---

## ğŸ“¡ API Documentation

### Base URL
```
Development: http://localhost:8000/api
Production: https://your-domain.com/api
```

### Endpoints

#### 1. Health Check
```http
GET /api/
```
**Response**:
```json
{ "message": "The Mirror Note API - Voice Assessment Platform" }
```

#### 2. Analyze Voice
```http
POST /api/analyze-voice
```
**Request**:
```json
{
  "audio_base64": "base64_encoded_audio_string",
  "user_id": "user_123",
  "recording_mode": "free",
  "recording_time": 30
}
```
**Response**:
```json
{
  "assessment_id": "uuid-v4",
  "status": "completed",
  "message": "Analysis completed successfully"
}
```

**Processing Steps**:
1. Decode base64 audio
2. Transcribe with OpenAI Whisper
3. Analyze with GPT-4 (archetype, scores, metrics)
4. Generate training questions
5. Store in MongoDB

#### 3. Get Assessment
```http
GET /api/assessment/{assessment_id}
```
**Response**:
```json
{
  "assessment_id": "uuid-v4",
  "user_id": "user_123",
  "processed": true,
  "transcription": "Your transcribed speech...",
  "analysis": {
    "archetype": "Confident Presenter",
    "overall_score": 85,
    "clarity_score": 88,
    "confidence_score": 82,
    "tone": "Professional",
    "strengths": ["Clear articulation", "Consistent pacing"],
    "improvements": ["Reduce filler words", "Vary tone"],
    "speaking_pace": 145,
    "pitch_avg": 165,
    "pitch_range": "Medium",
    "filler_words": { "um": 3, "like": 5 },
    "filler_count": 8,
    "word_count": 75
  },
  "training_questions": [
    {
      "question": "How can I reduce filler words?",
      "answer": "Practice pausing instead...",
      "is_free": true
    }
  ]
}
```

### Production Endpoints
See `PRODUCTION_INTEGRATION_GUIDE.md` for:
- `POST /api/auth/session` - Google OAuth
- `GET /api/auth/me` - Get current user
- `POST /api/payment/create-order` - Razorpay
- `POST /api/payment/verify` - Payment verification

---

## ğŸ§ª Testing

### Backend Tests

```bash
# Run automated test suite
cd backend
python ../backend_test.py
```

**Tests include**:
- âœ… Health check endpoint
- âœ… MongoDB connectivity
- âœ… Voice analysis with mock audio
- âœ… Assessment retrieval
- âœ… Error handling

### Frontend Testing

**Manual Test Flow**:
1. Launch â†’ Splash screen
2. Login â†’ Email/name entry
3. Dashboard â†’ View stats
4. Record â†’ Choose mode, record voice (up to 2 min)
5. Process â†’ Watch analysis stages
6. Results â†’ View scores, charts, training questions
7. Payment â†’ Mock subscription flow
8. History â†’ View past assessments

**Test on**:
- iOS Simulator (macOS)
- Android Emulator
- Physical device (Expo Go)
- Web browser (limited audio support)

### Interactive API Docs
```
http://localhost:8000/docs
```

---

## ğŸ” Production Integration

### Google Authentication (5 minutes)

**Production-ready** using Emergent Auth service.

```bash
# Backend
pip install emergentintegrations --extra-index-url https://d33sy5i8bnduwe.cloudfront.net/simple/
```

1. Add `auth.py` to backend
2. Update frontend `AuthContext.tsx`
3. Configure deep linking in `app.json`:
```json
{ "expo": { "scheme": "mirrornote" } }
```

**Flow**: User â†’ Google Auth â†’ App with session â†’ Backend verifies â†’ httpOnly cookie

### Razorpay Payments (10 minutes)

**Payment UI implemented** with mock flow. Production integration ready.

1. Get credentials from [Razorpay Dashboard](https://dashboard.razorpay.com/)
2. Install: `pip install razorpay` + `yarn add react-native-razorpay`
3. Add to `.env`:
```bash
RAZORPAY_KEY_ID=rzp_live_xxxxx
RAZORPAY_KEY_SECRET=xxxxx
RAZORPAY_WEBHOOK_SECRET=whsec_xxxxx
```
4. Add `payment.py` to backend
5. Configure webhooks

**Plans**: Monthly â‚¹499 | Yearly â‚¹3,999  
**Test Cards**: Success `4111 1111 1111 1111` | Fail `4000 0000 0000 0002`

**Full Details**: See `PRODUCTION_INTEGRATION_GUIDE.md`

---

## ğŸš¢ Deployment

### Backend

**Option 1: Emergent Agent Platform**
- Deploy via CLI/dashboard
- Set environment variables
- URL: `https://your-app.preview.emergentagent.com`

**Option 2: Traditional Hosting** (Heroku, Railway, Render)
```bash
# Example: Railway
npm install -g @railway/cli
railway login
railway init
railway up
```

### Frontend

**Option 1: EAS (iOS & Android)**
```bash
npm install -g eas-cli
eas login
eas build:configure
eas build --platform all
eas submit --platform ios
eas submit --platform android
```

**Option 2: Web**
```bash
npx expo export:web
# Deploy web-build/ to Netlify/Vercel
```

---

## ğŸ¨ Design System

### Color Palette (Sage Green Theme)

```typescript
const COLORS = {
  primary: '#8A9A5B',       // Sage green
  primaryDark: '#6B7A3F',   // Darker sage
  background: '#F8F9F5',    // Off-white green tint
  textPrimary: '#2D3319',   // Dark olive
  error: '#D32F2F',
  success: '#4CAF50',
};
```

### Typography & Spacing
```typescript
FONT_SIZES: { xs: 12, sm: 14, md: 16, lg: 18, xl: 24, xxl: 32 }
SPACING: { xs: 4, sm: 8, md: 16, lg: 24, xl: 32 }
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

**Standards**:
- Frontend: TypeScript, functional components, theme system
- Backend: PEP 8, type hints, docstrings
- Test all features before PR
- Update documentation

---

## ğŸ†˜ Support & Troubleshooting

### Common Issues

**Backend won't start**:
- Check Python 3.10+: `python --version`
- Verify `.env` variables
- Test MongoDB connection

**Frontend won't build**:
- Clear cache: `npx expo start -c`
- Reinstall: `rm -rf node_modules && npm install`
- Check Node 18+: `node --version`

**Audio not working**:
- Check microphone permissions
- Test on physical device
- Ensure HTTPS in production

**OpenAI errors**:
- Verify API key validity
- Check billing/quota limits
- Ensure model names: `whisper-1`, `gpt-4`

### Documentation
- Production Integration: `PRODUCTION_INTEGRATION_GUIDE.md`
- Quick Start: `QUICK_START_PRODUCTION.md`
- Test Results: `test_result.md`
- API Docs: `http://localhost:8000/docs`

---

## ğŸ—ºï¸ Roadmap

### Current (v1.0.0)
- âœ… Voice recording (free & guided)
- âœ… AI transcription & analysis
- âœ… Results dashboard with charts
- âœ… Training questions
- âœ… Premium UI (mocked)
- âœ… User authentication
- âœ… Assessment history

### Upcoming
- [ ] Social sharing features
- [ ] Progress comparison over time
- [ ] Advanced analytics & trends
- [ ] Live AI coaching sessions
- [ ] Multilingual support
- [ ] Team/corporate plans
- [ ] API access for developers
- [ ] Podcast/meeting analysis

---

## ğŸ“„ License

This project is proprietary software. All rights reserved.

**Copyright Â© 2024 The Mirror Note**

Unauthorized copying, distribution, or modification is prohibited.

---

## ğŸ‘¥ Credits

- **Developer**: Shaurya
- **Platform**: Emergent Agent
- **AI**: OpenAI (Whisper + GPT-4)
- **Frameworks**: Expo, FastAPI

---

## ğŸ¯ Quick Links

- [Getting Started](#-getting-started)
- [API Documentation](#-api-documentation)
- [Production Integration Guide](PRODUCTION_INTEGRATION_GUIDE.md)
- [Quick Start Guide](QUICK_START_PRODUCTION.md)
- [Test Documentation](test_result.md)
- [API Interactive Docs](http://localhost:8000/docs)

---

<div align="center">

**Made with â¤ï¸ using Expo, FastAPI, and OpenAI**

[â­ Star this repo](https://github.com/yourusername/mirrornote-emergent) | [ğŸ› Report Bug](https://github.com/yourusername/mirrornote-emergent/issues) | [ğŸ’¡ Request Feature](https://github.com/yourusername/mirrornote-emergent/issues)

</div>
